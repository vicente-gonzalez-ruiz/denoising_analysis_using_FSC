\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Denoising of Electron Microscopy 3D Data}

\author{Vicente González Ruiz and José Jesús Fernández Rodríguez}

\begin{document}
\maketitle

\begin{abstract}
  We analyze the effect of some denoising algorithms (GD, SPGD, AND,
  Cryo-CARE, BM4D and RSVD) applied to electron microscopy volumes,
  using different objective metrics (MSE, PPC, SSIM and FSC). Such
  volumes are noisy, and we don't have a denoised version of them to
  compare with. For this reason, for the same 
\end{abstract}

\section{Metrics}

The following metrics have been used to compare the noisy (original)
volume and the denoised volume.

\subsection{\href{https://en.wikipedia.org/wiki/Mean_squared_error}{MSE (Mean
    Squared Error)}}

Having two volumes $\mathbf{X}$ and $\mathbf{Y}$,
\begin{equation}
  \text{MSE}(\mathbf{X},\mathbf{Y}) = \frac{1}{N}\sum_{i=1}^N(\mathbf{X}_i - \mathbf{Y}_i)^2,
\end{equation}
where $N$ is the number of samples in one of the volumes (that must
have the same number of samples).

\subsection{\href{https://en.wikipedia.org/wiki/Structural_similarity_index_measure}{SSIM
    (Structural Similarity Index Measure)}}

After splitting the volume into $M$ non overlapped sub-vols,
\begin{equation}
  \text{SSIM}(\mathbf{X}, \mathbf{Y}) := \frac{1}{N} \sum_{i=1}^N \frac{(2\overline{\mathbf{x}}_i \overline{\mathbf{y}}_i + c_1)(2\sigma_{\mathbf{x}_i \mathbf{y}_i} + c_2)}{(\overline{\mathbf{x}_i^2} + \overline{\mathbf{y}_i^2} + c_1)(\sigma^2_{\mathbf{x}_i} + \sigma^2_{\mathbf{y}_i} + c_2)},
\end{equation}
where $\overline{x}i$ is the mean of the $i$-th sub-vol of
$\mathbf{X}$, $\sigma^2_{\mathbf{x}_i}$ is its variance (equivalently
for $\mathbf{Y}$), $\sigma_{\mathbf{x}_i\mathbf{y}_i}$ is the
covariance of both sub-vols, $c_1=(k_1L) ²$, $c_2=(k_2L) 2$ are two
variables to stabilize the division with weak denominator, $L$ is the
dynamic range of the voxel values, with constants with default vales
$K_1=0.01$ , and $k_2=0.03$, and where the default size of the local
sub-vols is $7\times 7\times 7$.

\subsection{\href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{PPC
    (Pearson Correlation Coefficient)}}
The PPC is given by
\begin{equation}
  \text{PPC}(\mathbf{X}, \mathbf{Y}) := \frac{\sum_i(\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{Y}_i - \overline{\mathbf{Y}})}{\sqrt{\sum_i (\mathbf{X}_i - \overline{\mathbf{X}})^2 \sum_i (\mathbf{Y}_i - \overline{\mathbf{Y}})^2}}
\end{equation}

\subsection{FSC (Fourier Shell Correlation) curve}
The FSC curve is a measure of the similarity between two 3D volumes
represented in the Fourier domain \cite{verbeke2024self}. Each point
of the curve prepresents the correlation between two ``shells'' of
Fourier coefficients of both volumes. An advantage of the FSC over
other similarity metrics such as the MSE, the SSIM and the PPC is that
FSC values depend on the frequency, and this can be interesting in
some scenarios, such as microscopy, where the resolution of the
microscope if finite and known.

A FSC value of the FSC curve is determined by~\cite{verbeke2024self}
\begin{equation}
\text{FSC}(\mathbf{X}, \mathbf{Y}; r) := \frac{\sum_{i \in S_r} (\mathcal{F}(\mathbf{X)}_i \mathcal{F}(\mathbf{Y)}_i^*)}{\sqrt{\sum_{i \in S_r} |\mathcal{F}(\mathbf{X})_i|^2 \sum_{i \in S_r} |\mathcal{F}(\mathbf{Y})_i|^2}},
\end{equation}
where $i=(x, y, z)$ is a point (a coefficient in the Fourier domain)
of the surface of the sphere $S_r$ defined by $x^2+y^2+z^2=r^2$,
$\mathcal{F}(\cdot)$ is the Fourier transform of $\cdot$, and
$\cdot^*$ denotes the complex conjugate of $\cdot$.

In some fields such ashsingle particle electron cryo-microscopy
(cryo-EM), the FSC curve has become the universal resolution metric
and is used to assess the quality of a 3-D reconstruction
\cite{rosenthal2003optimal,scheres2012prevention}.

It is possible to compute the Self FSC (SFSC) by subsampling the data
\cite{koho2019fourier}, for example, splitting the volume in odd and
even slices, in each dimension (resulting a total of 6 subvolumes),
computing the FSC and averaging the correlations
\cite{verbeke2024self}. Another approach can split the data into two
random half volumes \cite{verbeke2024self}.

\section{Noise models}
\label{sec:noise_models}

\subsection{Additive}

Let $\mathbf{X}$ be a \emph{clean} (without noise, ground truth
usually unknown) tensor, and ${\mathbf N}^{(i)}$ the $i$-th (unknown)
noise tensor instance of the set $\{{\mathbf N}^{(i)}\}_{i=1}^I$. The
additive noise model defines a noisy tensor
\begin{equation}
  \hat{\mathbf X}^{(i)} = {\mathbf X} + {\mathbf N}^{(i)},
  \label{eq:additive_noisy_model}
\end{equation}
where
\begin{equation}
  {\mathbb E}(I)[{\mathbf N}^{(i)}_j]=\frac{1}{I} \sum_{i=1}^I {\mathbf N}_j^{(i)}=0,
  \label{eq:noise_expectation}
\end{equation}
where ${\mathbf N}^{(i)}_j$ is the $j$-th random noise (scalar) value
of ${\mathbf N}^{(i)}$, of a total of $J$ per noisy tensor
instance. Notice that if the ${\mathbf N}^{(i)}_j$ are
statistically i.i.d. (independent and identically distributed), it is
hold that
\begin{equation}
  \lim_{I \to \infty}{\mathbb E}(I)[{\mathbf N}]=\lim_{I \to \infty}\frac{1}{I} \sum_{i=1}^I {\mathbf N}^{(i)}={\mathbf 0},
  \label{eq:noise_expectation_2}
\end{equation}
(where ${\mathbf 0}$ is the zero tensor) and that
\begin{equation}
  \lim_{I \to \infty} \mathbb{E}(I)\left[\hat{\mathbf X}\right] = \lim_{I \to \infty} \frac{1}{I} \sum_{i=1}^I \hat{\mathbf X}^{(i)} = {\mathbf X}.
  \label{eq:averaging_result}
\end{equation}

We assume that ${\mathbf X}$ and $\mathbf{N}$ are statistically
independent, and therefore, nothing can be said about
${\mathbf N}^{(i)}$ from $\hat{\mathbf X}^{(i)}$ $\forall i$, and
viceversa. For this reason, it is impossible to obtain ${\mathbf X}$
from $\hat{\mathbf X}$, althouth we can approximate ${\mathbf X}$ by
averaging $\hat{\mathbf X}^{(i)}$ instances (see
Eq.~\ref{eq:averaging_result}).

Examples of zero-mean i.i.d. noise distributions that
satisfy\footnote{See
  \url{https://github.com/vicente-gonzalez-ruiz/denoising_analysis_using_FSC/tree/main/figs/average_denoising.ipynb}}
Eq.~\ref{eq:averaging_result} are:
\begin{enumerate}
\item Zero-mean uniform noise (i.e., ${\mathbf N}\sim{\mathcal U}(c)$) with
  PDF (Probability Density Function) defined as
  \begin{equation}
    f(x; c) = \Pr({\mathbf N}^{(i)}_j{=}x) = \begin{cases}
      \frac{1}{2c} & \text{for } -c \le x \le c, \\[8pt]
      0 & \text{for } x < c \ \text{ or } \ x > -c,
    \end{cases}
  \end{equation}
  $x$ and $f$ continuous.
\item Zero-mean Gaussian noise
  (${\mathbf N}\sim{\mathcal N}(\mu=0,\sigma^2)$, representing
  $\sigma$ the variance of the noise) with PDF defined by
  \begin{equation}
    f(x; \sigma) = \Pr({\mathbf N}^{(i)}_j{=}x) = \frac 1 {\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2} },
  \end{equation}
  $x$ and $f$ continuous.
\end{enumerate}

\subsection{Multiplicative (lineal)}
In multiplicative models, the noisy tensor is modeled by
\begin{equation}
  \hat{\mathbf X}^{(i)} = {\mathbf X} (1 + {\mathbf N}^{(i)}).
  \label{eq:multiplicative_noisy_model}
\end{equation}

As can be seen, the effective noise added to ${\mathbf X}$ is
${\mathbf X}{\mathbf N}^{(i)}$, and for this reason, it can be said
that multiplicative models are signal dependent (${\mathbf X}$
\emph{modulates} the amount of noise that affect to the each signal
sample of ${\mathbf X}$, even if the random variable
${\mathbf N}^{(i)}$ is i.i.d.). Luckily, this fact does not pose a
major inconvenience to continue satisfying
Eq.~\ref{eq:noise_expectation}, (and therefore, also
Eq.~\ref{eq:averaging_result}) if the variation in intensity due to
noise that each sample suffers between different tensor instances (the
standard deviation) has zero mean.

Examples of multiplicative noise distributions:

\begin{enumerate}
\item Poisson noise ($\mathbf{N}\sim\mathcal{P}(\lambda)$), also
  called \emph{shot noise}, is defined by a PMD (Probability Mass
  Distribution)\footnote{Also called discrete PDF.} equal to
  \begin{equation}
    f(k; \lambda) = \Pr({\mathbf N}^{(i)}_j{=}k) = \frac{\lambda^k e^{-\lambda}}{k!},
  \end{equation}
  both $f$ and $k$ discretes ($f$ describes the probability of $k$
  events occurring within the observed interval $\lambda$).  ..
\end{enumerate}

Notice that multiplicative noise models are signal-dependent (depends
on the amplitdude of the clean signal).

\subsection{Non-lineal}

There exist noise models in which the amplitude of the noise depends
on the amplitude of the signal (are signal-dependent), but the
relationship between the signal and the noise is not lineal.

Examples of non-lineal noise distributions:

\begin{enumerate}
\item Rician noise ($\mathbf{N}\sim\mathcal{R}(\sigma)$), with PDF
  \begin{equation}
    f(x\mid\nu,\sigma) = \frac{x}{\sigma^2}e^{\frac{-(x^2+\nu^2)}{2\sigma^2}}I_0\left(\frac{x\nu}{\sigma^2}\right),
  \end{equation}
  where $x$ is continuous. Rician noise is modeled by
\begin{equation}
  \hat{\mathbf X}^{(i)} = \sqrt{ ({\mathbf X} + {\mathbf N}_{\text{real}}^{(i)})^2 + ({\mathbf N}_{\text{imag}}^{(i)})^2}, \hat{\mathbf X}^{(i)}\sim\text{Rice}(\nu,\sigma)
\end{equation}
\end{enumerate}


\subsection{Expectation of noisy instances}
In all the noise models described in Section~\ref{sec:noise_models}
the random tensors $\{{\mathbf N}_i\}_{i=1}^N$ are i.i.d., and therefore,
it's hold that




\section{GD (Gaussian Denoising)}

Also known by Gaussian smoothing filtering.

Gaussian denoising works weight-averaging signal samples. Under the
assumption that most of the energy (and information) is concentrated
in the low frequencies, a Gaussian filter is defined by
\begin{equation}
  \mathbf{h}(\sigma) = \{\mathbf{h}_i(\sigma)\} = \frac{1}{\sqrt{2\pi}\sigma}e^{{-i}^2/(2\sigma^2)},
\end{equation}
where $\sigma$ is the standard deviation of a normal
distribution. 1-D Gaussian filtering is represented by
\begin{equation}
  \hat{\mathbf{Y}}(\sigma) = \mathbf{Y}*\mathbf{h}(\sigma),
  \label{eq:GF}
\end{equation}
where $\cdot*\cdot$ represent the convolution of digital 1-D signals.

Multidimensional Gaussian filters are separable, which means that we
can apply the 1D filter to all the dimensions of the signal to compute
a $N$-D denoising:
\begin{equation}
  \hat{\mathbf{Y}}(\sigma) = \Big(\big({\mathbf Y}*^{(\text{Z})}{\mathbf h}(\sigma)\big)*^{(\text{Y})}{\mathbf h}(\sigma)\Big)*^{(\text{X})}{\mathbf h}(\sigma),
    \label{eq:3DGF}
\end{equation}
where ${\mathbf s}*^{(d)}{\mathbf h}$ is the 1D convolution applied to
the dimension $d$ of the signal ${\mathbf s}$ and the 1D filter
${\mathbf h}$:
\begin{equation*}
    \begin{array}{l}
    \mathbf{Y}*^{(\text{Z})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[:,y,x]}*{\mathbf h}(\sigma)~\text{for}~y,x=0,1,\cdots,N-1\},\\
    \mathbf{Y}*^{(\text{Y})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[z,:,x]}*{\mathbf h}(\sigma)~\text{for}~z,x=0,1,\cdots,N-1\}, \text{and}\\
    \mathbf{Y}*^{(\text{X})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[z,y,:]}*{\mathbf h}(\sigma)~\text{for}~z,y=0,1,\cdots,N-1\}.
    \end{array}
\end{equation*}

For simplicity, Eq.~\ref{eq:3DGF} defines isotropic
filtering, but variance can be different at each dimension to
provide anisotropy.

GD requires $\sigma$ to be provided as a parameter. A higher $\sigma$
results in a lower the cut-off frequency of the filter. Notice that
$\sigma$ is not (objectively) optimizable using only distortion 
metrics.

\section{SDPG (Structure-Preserving Gaussian Denoising) \cite{gonzalez2023structure}}

SDPG is based in 3D Gaussian filtering (Eq.~\ref{eq:3DGF}), but the noised
volume ${\mathbf Y}$ is dynamically warped to decrease the bluring at the
structures detected by an 2-D OF (Optical Flow) estimator:
\begin{equation}
  \hat{\mathbf{Y}}(\sigma; w, l) = \Big(\big(R_\text{Z}(\mathbf{Y}; w, l)*^{(\text{Z})}{\mathbf h}(\sigma)\big)*^{(\text{Y})}{\mathbf h}(\sigma)\Big)*^{(\text{X})}{\mathbf h}(\sigma),
    \label{eq:SDPG}
\end{equation}
where
\begin{equation*}
    \begin{array}{rclll}
    R_\text{Z}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{z'\rightarrow z}{\mathbf d}({\mathbf Y}_{[z',:,:]}; w, l)~:~\overset{z'\rightarrow z}{\mathbf d}({\mathbf Y}_{[z',:,:]})\approx{\mathbf Y}_{[z,:,:]} & \\ & & \text{for}
 ~z'=z-k,\cdots,z+k\} ~\text{for}~z=0,1,\cdots,N_\text{Z}-1\big\}, \\
    R_\text{Y}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{y'\rightarrow y}{\mathbf d}({\mathbf Y}_{[:,y',:]}; w, l)~:~\overset{y'\rightarrow y}{\mathbf d}({\mathbf Y}_{[:,y',:]}; w, l)\approx{\mathbf Y}_{[:,y,:]} & \\ & & \text{for}
 ~y'=y-k,\cdots,y+k\} ~\text{for}~y=0,1,\cdots,N_\text{Y}-1\big\},~\text{and} \\
    R_\text{X}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)~:~\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)\approx{\mathbf Y}_{[:,:,x]} & \\ & & \text{for}
 ~x'=x-k,\cdots,x+k\} ~\text{for}~x=0,1,\cdots,N_\text{X}-1\big\}
    \end{array}
\end{equation*}
are the warped volumes. For example,
$\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)$
represents the projection of the slice at coordinate $x'$ fulfilling
that
$\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w,
l)\approx{\mathbf Y}_{[:,:,x]}$. Notice that, for each possible offset
in ${\mathbf Z}$, ${\mathbf Y}$, and ${\mathbf X}$, a different set of
warped 2-D slices must be computed.

As indicated in Eq.~\ref{eq:SDPG}, the estimator requires 2 new
parameters:
\begin{enumerate}
\item $w$, the size of a 2D window used to analyze the 2D slices. If
  $w$ is large, the estimator is less sensitive to the noise but also
  the small structures are ignored. Therefore, higher $w$ values
  increases blurring.
\item $l$, that controls the maximun displacements that the
  estimator can generate. Blurring increases with $l$.
\end{enumerate}


% \section{AND}

% \section{N2V}
% A. Krull, T.-O. Buchholz, and F. Jug, “Noise2void-learning denoising
% from single noisy images,” in Proceedings of the IEEE/CVF conference
% on computer vision and pattern recognition, 2019, pp. 2129–2137.

\section{Cryo-CARE \cite{buchholz2019cryo}}
CARE (Content-Aware image REstoration) training methods leverage
available knowledge about the data at hand ought to yield superior
restoration results \cite{weigert2018content}. Concretely, Cryo-CARE
is an implementation of Noise2Noise (N2N) \cite{lehtinen2018noise2noise}.

N2N is a ``supervised'' learning method for denoising where the model
(a U-Net) is trained on pairs of noisy images. However, unlike
clasical supervised denoising deep-learning based models, that
implement
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j\big)
\end{equation}
where $\{(\hat{\mathbf X}_j^{(1)}, {\mathbf X}_j)\}_{j=1}^M$ is the training
dataset, and $L$ is a given lost function such as the MSE (here higher
values mean worst results), N2N solves
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j^{(2)}\big).
\end{equation}
In other words, given two noisy versions
$\{\hat{\mathbf Y}^{(1)}, \hat{\mathbf Y}^{(2)}\}$ of the same (clean)
volume ${\mathbf Y}$, N2N learns to infeer a denoised volume
\begin{equation}
  \tilde{\mathbf Y}=\frac{1}{2}\big(f_\theta(\hat{\mathbf Y}^{(1)})+f_\theta(\hat{\mathbf Y}^{(2)})\big)\approx{\mathbf Y}.
\end{equation}
Obviously, better approximations to ${\mathbf Y}$ will be obtained
having more noisy instances, after averaing all the denoised volumes.

\section{NLM (Non Local Means)} 

\section{BM4D}

\section{2D-RSVD (2D Random Shuffing Volume Denoising}

\section{3D-RSVD (3D Random Shuffling Volume Denoising}


\section{}

\bibliographystyle{plain}
\bibliography{signal_processing,microscopy,denoising}

\end{document}
