\documentclass{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsfonts}

\title{Denoising in Electron Microscopy Imaging}

\author{Vicente González-Ruiz and José Jesús Fernández Rodríguez}

\begin{document}
\maketitle

\begin{abstract}
  We analyze the denoising performance of signal averaging for several
  noise statistical distributions. Then, we perform an objective
  comparison between some of the denoising algoritms in the field
  of 3D Electron Microscopy, in two situations, when we have a
  denoised version of the signal and we have not.
\end{abstract}

\section{Signal denoising by averaging}
Let $\mathbf{X}$ be a \emph{clean} (without noise, ground truth
usually unknown) tensor\footnote{Which usually represents an image or
  a volume.}, and $\hat{\mathbf X}^{(i)}$ the $i$-th noisy-version
tensor random instance of $\mathbf{X}$ generated, in general, through
\begin{equation}
  \hat{\mathbf X}^{(i)} = f(\mathbf{X}, \mathbf{N}^{(i)}),
\end{equation}
where ${\mathbf N}^{(i)}$ the $i$-th (unknown) noise tensor instance
with the same shape as $\mathbf{X}$.

We assume that ${\mathbf X}$ and $\mathbf{N}$ are statistically
independent, and therefore, nothing can be said about
${\mathbf N}^{(i)}$ if we know $\hat{\mathbf X}^{(i)}$, and
viceversa, and therefore, it is impossible to obtain ${\mathbf X}$
from a single $\hat{\mathbf X}^{(i)}$, althouth we can approximate ${\mathbf X}$ by
averaging $\hat{\mathbf X}^{(i)}$ instances, performing
\begin{equation}
  \lim_{I \to \infty} \mathbb{E}\left[\{\hat{\mathbf X}^{(i)}\}_{i=1}^I\right] = \lim_{I \to \infty} \frac{1}{I} \sum_{i=1}^I \hat{\mathbf X}^{(i)} = {\mathbf X}.
  \label{eq:averaging_result}
\end{equation}
Notice that Eq.~\ref{eq:averaging_result} is true if 
\begin{equation}
  \overline{\mathbf N} = \lim_{I \to \infty}{\mathbb E}[\{{\mathbf N^{(i)}}\}_{i=1}^I]=\lim_{I \to \infty}\frac{1}{I} \sum_{i=1}^I {\mathbf N}^{(i)}={\mathbf 0},
  \label{eq:noise_expectation_2}
\end{equation}
where ${\mathbf 0}$ is the zero tensor with the same shape as $\mathbf{X}$, and when $\overline{\mathbf N}\ne {\mathbf 0}$, but known, we can still find ${\mathbf X}$ through
\begin{equation}
  {\mathbf X} = \lim_{I \to \infty} \mathbb{E}\left[\{\hat{\mathbf X}^{(i)}\}_{i=1}^I\right]  - \overline{\mathbf N},
  \label{eq:averaging_result_with_bias}
\end{equation}
where, in general, $\overline{\mathbf N}$ can be a function of the amplitude of the noise. 

\section{Noise models}
\label{sec:noise_models}

\subsection{Signal-independent noise (SIN) models}
In SIN models, the noise is statistically i.i.d. (independent and
identically distributed), i.e., the random values of ${\mathbf N}^{(i)}$ satisfy that
\begin{equation}
  {\mathbb E}[\{{\mathbf N}^{(i)}_j\}_{j=1}^J]=\frac{1}{J} \sum_{i=1}^J {\mathbf N}_j^{(i)}=\overline{\mathbf N}^{(i)},
  \label{eq:noise_expectation_1}
\end{equation}
where ${\mathbf N}^{(i)}_j$ is the $j$-th (scalar) value
of ${\mathbf N}^{(i)}$, and
\begin{equation}
  J=\prod_{k=1}^D \mathbf{X}.\text{shape}[k],
\end{equation}
where $D$ is the number of dimensions of the signal (in microscopy, usually 2 or 3).

SIN models, also called additive noise models, are defined by
\begin{equation}
  \hat{\mathbf X}^{(i)} = {\mathbf X} + {\mathbf N}^{(i)}.
  \label{eq:additive_noisy_model}
\end{equation}

Examples\footnote{See
  \url{https://github.com/vicente-gonzalez-ruiz/denoising_analysis_using_FSC/tree/main/figs/average_denoising.ipynb}}
of zero-mean i.i.d. noise distributions that model the noise typically
generated by signal digital-sampling devices are:
\begin{enumerate}
\item Zero-mean uniform noise (i.e., ${\mathbf N}\sim{\mathcal U}(c)$) with
  PDF (Probability Density Function) defined as
  \begin{equation}
    f(x; c) = \Pr({\mathbf N}^{(i)}_j{=}x) = \begin{cases}
      \frac{1}{2c} & \text{for } -c \le x \le c, \\[8pt]
      0 & \text{for } x < c \ \text{ or } \ x > -c,
    \end{cases}
  \end{equation}
  $x$ and $f$ continuous, where $\Pr(\cdot)$ represents the
  probability of $\cdot$, and $c$ controls the amplitude of the
  noise. Quantization noise is usually modeled by uniform
  noise. Notice that for this type of noise,
  $\overline{\mathbf{N}}^{(i)}=0$ and therefore,
  $\overline{\mathbf N}={\mathbf 0}$.
\item Zero-mean Gaussian noise
  (${\mathbf N}\sim{\mathcal N}(\mu=0,\sigma^2)$,  with PDF defined by
  \begin{equation}
    f(x; \sigma) = \Pr({\mathbf N}^{(i)}_j{=}x) = \frac 1 {\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2} },
  \end{equation}
  $x$ and $f$ continuous, representing $\mu$ the mean and $\sigma$ the
  variance of the noise. Thermal noise is usually modeled by Gaussian
  noise. Again, $\overline{\mathbf{N}}^{(i)}=0$ i.e.,
  $\overline{\mathbf N}={\mathbf 0}$.
\end{enumerate}

\subsection{Signal-dependent noise (SDN) models}
In SDN models, the amplitude of the random noise samples depends on the amplitude of the signal through
\begin{equation}
  \hat{\mathbf X}^{(i)} = \mathbf{X} + {\mathbf N}^{(i)}(\mathbf{X}),
  \label{eq:SDN_model}
\end{equation}
where as it can be seen, the noise instance if a function of the clean
sinal.

Examples of SDN cases are:
\begin{enumerate}
\item Multiplicative Gaussian noise, expresed as
  \begin{equation}
    \hat{\mathbf X}^{(i)} = {\mathbf X} (1 + {\mathbf N}^{(i)}),
    \label{eq:MGN}
  \end{equation}
  where ${\mathbf N}\sim{\mathcal N}(\mu,\sigma)$. \emph{Speckle
    noise} can be modeled by multiplicative Gaussian noise if
  $\mu=0$. The noise present in synthetic aperture radar (SAR) and
  ultrasound images is usually considered multiplicative Gaussian
  noise.

\item Another distribution used for modeling speckle noise is the Rice
  distribution ($\mathbf{N}\sim\mathrm{Rice}(\nu,\sigma)$), with PDF
  \begin{equation}
    f(x; \nu,\sigma) = \Pr({\mathbf N}^{(i)}_j{=}x) = \frac{x}{\sigma^2}e^{\frac{-(x^2+\nu^2)}{2\sigma^2}}I_0\left(\frac{x\nu}{\sigma^2}\right),
  \end{equation}
  where $x$ is continuous. Rician noisy tensor instances can be generated with
  \begin{equation}
    \hat{\mathbf X}^{(i)} = \sqrt{ ({\mathbf X} + {\mathbf N}_{\text{real}}^{(i)})^2 + ({\mathbf N}_{\text{imag}}^{(i)})^2}, \hat{\mathbf X}^{(i)}.
  \end{equation}
  %Notice that the Rayleigh distribution ($\mathbf{N}\sim\mathrm{Rayleigh}(\sigma)$),
  %which is defined by the PDF
  %\begin{equation}
  %  {\mathbf N}^{(i)} \sim f(x; \sigma) = \frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)}, \quad x \geq 0,
  %\end{equation}
  %continuous both, $x$ and $\sigma$ (the scale parameter) is a
  %particular case of Rice distribution when $\nu=0$.
  Notice that, even being $\nu=0$ (in whose case we are
  working with the Rayleigh distribution
  ($\mathbf{N}\sim\mathrm{Rayleigh}(\sigma)$)), the mean of the noise is not zero.

  The noise that corrupts magnetic resonance images is usually modeled as Rician noise.

\item Poisson noise ($\mathbf{N}\sim\mathrm{Poisson}(\lambda)$), also
  called \emph{shot noise}, where the noisy instance satisfies a PMD
  (Probability Mass Distribution)\footnote{Also called discrete PDF.}
  equal to
  \begin{equation}
    {\mathbf N}^{(i)} \sim f(k; \lambda={\mathbf X}) = \Pr({\mathbf N}^{(i)}{=}k) = \frac{\lambda^k e^{-\lambda}}{k!},
    \label{eq:PN}
  \end{equation}
  where both $f\in\mathbb{R}$, $k\in\mathbb{Z}$ are discretes (for
  example, $f$ describes the probability of $k$ events occurring
  within a observed interval (of time) when, in average,
  we have $\lambda\in\mathbb{R}$ events in such interval
  size). Poisson noise can be significative in optical capturing
  devices that work with low doses of photons, such as often happens
  in Electron Microscopy.


\end{enumerate}

  
  \begin{equation}
    \hat{\mathbf X}^{(i)} = {\mathbf X} + \mathrm{Poisson}(\lambda={\mathbf X}),
    \label{eq:PN}
  \end{equation}


  is defined by a PMD (Probability Mass
  Distribution)\footnote{Also called discrete PDF.} equal to
  \begin{equation}
    f(k; \lambda) = \Pr({\mathbf N}^{(i)}_j{=}k) = \frac{\lambda^k e^{-\lambda}}{k!},
  \end{equation}
  both $f$, $k\in\mathbb{Z}$ discretes (for example, $f$ describes the
  probability of $k$ events occurring within a observed interval (of
  time) when, in average, we have $\lambda\in\mathbb{R}$ events in
  such interval size). Poisson noise can be significative in optical
  capturing devices when work with low doses of photons, such as in
  Electron Microscopy.

In multiplicative models, the noisy tensor is defined by
\begin{equation}
  \hat{\mathbf X}^{(i)} = {\mathbf X} (1 + {\mathbf N}^{(i)}),
  \label{eq:multiplicative_noisy_model}
\end{equation}
and for this reason, SND models are also known as multiplicative noise
models. As can be seen, ${\mathbf X}$ \emph{modulates} the amount of
noise that affect to the each signal sample of ${\mathbf X}$, even if
the random variable ${\mathbf N}^{(i)}$ is i.i.d..

As it can be seen, the effective noise added to ${\mathbf X}$ is
${\mathbf X}{\mathbf N}^{(i)}$, and for this reason, it can be said
that multiplicative models are signal dependent (${\mathbf X}$
\emph{modulates} the amount of noise that affect to the each signal
sample of ${\mathbf X}$, even if the random variable
${\mathbf N}^{(i)}$ is i.i.d.). Luckily, this fact does not pose a
major inconvenience to continue satisfying
Eq.~\ref{eq:noise_expectation_1}, (and therefore, also
Eq.~\ref{eq:averaging_result}) if the variation in intensity due to
noise that each sample suffers between different tensor instances (the
standard deviation) has zero mean.

An example of multiplicative zero-mean noise distribution is:

\begin{enumerate}
\item Poisson noise ($\mathbf{N}\sim\mathcal{P}(\lambda)$), also
  called \emph{shot noise}, is defined by a PMD (Probability Mass
  Distribution)\footnote{Also called discrete PDF.} equal to
  \begin{equation}
    f(k; \lambda) = \Pr({\mathbf N}^{(i)}_j{=}k) = \frac{\lambda^k e^{-\lambda}}{k!},
  \end{equation}
  both $f$, $k\in\mathbb{Z}$ discretes (for example, $f$ describes the
  probability of $k$ events occurring within a observed interval (of
  time) when, in average, we have $\lambda\in\mathbb{R}$ events in
  such interval size). Poisson noise can be significative in optical
  capturing devices when work with low doses of photons, such as in
  Electron Microscopy.
\end{enumerate}

Notice that multiplicative noise models are signal-dependent (noise depends
on the amplitude of the clean signal).

\subsection{Non-lineal noise models}

There exist noise models in which the amplitude of the noise depends
on the amplitude of the signal (are signal-dependent), but the
relationship between the signal and the noise is not lineal.

Examples of non-lineal noise distributions:

\begin{enumerate}
\item Rayleigh noise ($\mathbf{N}\sim\mathrm{Rayleigh}(\sigma)$),
  also called \emph{speckle noise}, is defined by the PDF
  \begin{equation}
    f(x; \sigma) = \frac{x}{\sigma^2} e^{-x^2/(2\sigma^2)}, \quad x \geq 0,
  \end{equation}
  continuous both, $x$ and $\sigma$ (the scale parameter). Rayleigh
  distribution particular case of Rice distribution
\item Rician noise ($\mathbf{N}\sim\mathcal{R}(\sigma)$), with PDF
  \begin{equation}
    f(x\mid\nu,\sigma) = \frac{x}{\sigma^2}e^{\frac{-(x^2+\nu^2)}{2\sigma^2}}I_0\left(\frac{x\nu}{\sigma^2}\right),
  \end{equation}
  where $x$ is continuous. Rician noise is modeled by
\begin{equation}
  \hat{\mathbf X}^{(i)} = \sqrt{ ({\mathbf X} + {\mathbf N}_{\text{real}}^{(i)})^2 + ({\mathbf N}_{\text{imag}}^{(i)})^2}, \hat{\mathbf X}^{(i)}\sim\text{Rice}(\nu,\sigma)
\end{equation}
\end{enumerate}


\subsection{Expectation of noisy instances}
In all the noise models described in Section~\ref{sec:noise_models}
the random tensors $\{{\mathbf N}_i\}_{i=1}^N$ are i.i.d., and therefore,
it's hold that





\section{GD (Gaussian Denoising)}

Also known by Gaussian smoothing filtering.

Gaussian denoising works weight-averaging signal samples. Under the
assumption that most of the energy (and information) is concentrated
in the low frequencies, a Gaussian filter is defined by
\begin{equation}
  \mathbf{h}(\sigma) = \{\mathbf{h}_i(\sigma)\} = \frac{1}{\sqrt{2\pi}\sigma}e^{{-i}^2/(2\sigma^2)},
\end{equation}
where $\sigma$ is the standard deviation of a normal
distribution. 1-D Gaussian filtering is represented by
\begin{equation}
  \hat{\mathbf{Y}}(\sigma) = \mathbf{Y}*\mathbf{h}(\sigma),
  \label{eq:GF}
\end{equation}
where $\cdot*\cdot$ represent the convolution of digital 1-D signals.

Multidimensional Gaussian filters are separable, which means that we
can apply the 1D filter to all the dimensions of the signal to compute
a $N$-D denoising:
\begin{equation}
  \hat{\mathbf{Y}}(\sigma) = \Big(\big({\mathbf Y}*^{(\text{Z})}{\mathbf h}(\sigma)\big)*^{(\text{Y})}{\mathbf h}(\sigma)\Big)*^{(\text{X})}{\mathbf h}(\sigma),
    \label{eq:3DGF}
\end{equation}
where ${\mathbf s}*^{(d)}{\mathbf h}$ is the 1D convolution applied to
the dimension $d$ of the signal ${\mathbf s}$ and the 1D filter
${\mathbf h}$:
\begin{equation*}
    \begin{array}{l}
    \mathbf{Y}*^{(\text{Z})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[:,y,x]}*{\mathbf h}(\sigma)~\text{for}~y,x=0,1,\cdots,N-1\},\\
    \mathbf{Y}*^{(\text{Y})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[z,:,x]}*{\mathbf h}(\sigma)~\text{for}~z,x=0,1,\cdots,N-1\}, \text{and}\\
    \mathbf{Y}*^{(\text{X})}{\mathbf h}(\sigma)=\{{\mathbf Y}_{[z,y,:]}*{\mathbf h}(\sigma)~\text{for}~z,y=0,1,\cdots,N-1\}.
    \end{array}
\end{equation*}

For simplicity, Eq.~\ref{eq:3DGF} defines isotropic
filtering, but variance can be different at each dimension to
provide anisotropy.

GD requires $\sigma$ to be provided as a parameter. A higher $\sigma$
results in a lower the cut-off frequency of the filter. Notice that
$\sigma$ is not (objectively) optimizable using only distortion 
metrics.

\section{SDPG (Structure-Preserving Gaussian Denoising) \cite{gonzalez2023structure}}

SDPG is based in 3D Gaussian filtering (Eq.~\ref{eq:3DGF}), but the noised
volume ${\mathbf Y}$ is dynamically warped to decrease the bluring at the
structures detected by an 2-D OF (Optical Flow) estimator:
\begin{equation}
  \hat{\mathbf{Y}}(\sigma; w, l) = \Big(\big(R_\text{Z}(\mathbf{Y}; w, l)*^{(\text{Z})}{\mathbf h}(\sigma)\big)*^{(\text{Y})}{\mathbf h}(\sigma)\Big)*^{(\text{X})}{\mathbf h}(\sigma),
    \label{eq:SDPG}
\end{equation}
where
\begin{equation*}
    \begin{array}{rclll}
    R_\text{Z}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{z'\rightarrow z}{\mathbf d}({\mathbf Y}_{[z',:,:]}; w, l)~:~\overset{z'\rightarrow z}{\mathbf d}({\mathbf Y}_{[z',:,:]})\approx{\mathbf Y}_{[z,:,:]} & \\ & & \text{for}
 ~z'=z-k,\cdots,z+k\} ~\text{for}~z=0,1,\cdots,N_\text{Z}-1\big\}, \\
    R_\text{Y}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{y'\rightarrow y}{\mathbf d}({\mathbf Y}_{[:,y',:]}; w, l)~:~\overset{y'\rightarrow y}{\mathbf d}({\mathbf Y}_{[:,y',:]}; w, l)\approx{\mathbf Y}_{[:,y,:]} & \\ & & \text{for}
 ~y'=y-k,\cdots,y+k\} ~\text{for}~y=0,1,\cdots,N_\text{Y}-1\big\},~\text{and} \\
    R_\text{X}(\mathbf{Y}; w, l) & = & \big\{ \{ \overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)~:~\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)\approx{\mathbf Y}_{[:,:,x]} & \\ & & \text{for}
 ~x'=x-k,\cdots,x+k\} ~\text{for}~x=0,1,\cdots,N_\text{X}-1\big\}
    \end{array}
\end{equation*}
are the warped volumes. For example,
$\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w, l)$
represents the projection of the slice at coordinate $x'$ fulfilling
that
$\overset{x'\rightarrow x}{\mathbf d}({\mathbf Y}_{[:,:,x']}; w,
l)\approx{\mathbf Y}_{[:,:,x]}$. Notice that, for each possible offset
in ${\mathbf Z}$, ${\mathbf Y}$, and ${\mathbf X}$, a different set of
warped 2-D slices must be computed.

As indicated in Eq.~\ref{eq:SDPG}, the estimator requires 2 new
parameters:
\begin{enumerate}
\item $w$, the size of a 2D window used to analyze the 2D slices. If
  $w$ is large, the estimator is less sensitive to the noise but also
  the small structures are ignored. Therefore, higher $w$ values
  increases blurring.
\item $l$, that controls the maximun displacements that the
  estimator can generate. Blurring increases with $l$.
\end{enumerate}


% \section{AND}

% \section{N2V}
% A. Krull, T.-O. Buchholz, and F. Jug, “Noise2void-learning denoising
% from single noisy images,” in Proceedings of the IEEE/CVF conference
% on computer vision and pattern recognition, 2019, pp. 2129–2137.

\section{Cryo-CARE \cite{buchholz2019cryo}}
CARE (Content-Aware image REstoration) training methods leverage
available knowledge about the data at hand ought to yield superior
restoration results \cite{weigert2018content}. Concretely, Cryo-CARE
is an implementation of Noise2Noise (N2N) \cite{lehtinen2018noise2noise}.

N2N is a ``supervised'' learning method for denoising where the model
(a U-Net) is trained on pairs of noisy images. However, unlike
clasical supervised denoising deep-learning based models, that
implement
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j\big)
\end{equation}
where $\{(\hat{\mathbf X}_j^{(1)}, {\mathbf X}_j)\}_{j=1}^M$ is the training
dataset, and $L$ is a given lost function such as the MSE (here higher
values mean worst results), N2N solves
\begin{equation}
  \underset{\theta}{\operatorname{arg\,min}} \, \sum_j L \big(f_\theta(\hat{\mathbf X}_j^{(1)}), {\mathbf X}_j^{(2)}\big).
\end{equation}
In other words, given two noisy versions
$\{\hat{\mathbf Y}^{(1)}, \hat{\mathbf Y}^{(2)}\}$ of the same (clean)
volume ${\mathbf Y}$, N2N learns to infeer a denoised volume
\begin{equation}
  \tilde{\mathbf Y}=\frac{1}{2}\big(f_\theta(\hat{\mathbf Y}^{(1)})+f_\theta(\hat{\mathbf Y}^{(2)})\big)\approx{\mathbf Y}.
\end{equation}
Obviously, better approximations to ${\mathbf Y}$ will be obtained
having more noisy instances, after averaing all the denoised volumes.

\section{NLM (Non Local Means)} 

\section{BM4D}

\section{2D-RSVD (2D Random Shuffing Volume Denoising}

\section{3D-RSVD (3D Random Shuffling Volume Denoising}

\section{Metrics}

The following metrics have been used to compare the noisy (original)
volume and the denoised volume.

\subsection{\href{https://en.wikipedia.org/wiki/Mean_squared_error}{MSE (Mean
    Squared Error)}}

Having two volumes $\mathbf{X}$ and $\mathbf{Y}$,
\begin{equation}
  \text{MSE}(\mathbf{X},\mathbf{Y}) = \frac{1}{N}\sum_{i=1}^N(\mathbf{X}_i - \mathbf{Y}_i)^2,
\end{equation}
where $N$ is the number of samples in one of the volumes (that must
have the same number of samples).

\subsection{\href{https://en.wikipedia.org/wiki/Structural_similarity_index_measure}{SSIM
    (Structural Similarity Index Measure)}}

After splitting the volume into $M$ non overlapped sub-vols,
\begin{equation}
  \text{SSIM}(\mathbf{X}, \mathbf{Y}) := \frac{1}{N} \sum_{i=1}^N \frac{(2\overline{\mathbf{x}}_i \overline{\mathbf{y}}_i + c_1)(2\sigma_{\mathbf{x}_i \mathbf{y}_i} + c_2)}{(\overline{\mathbf{x}_i^2} + \overline{\mathbf{y}_i^2} + c_1)(\sigma^2_{\mathbf{x}_i} + \sigma^2_{\mathbf{y}_i} + c_2)},
\end{equation}
where $\overline{x}i$ is the mean of the $i$-th sub-vol of
$\mathbf{X}$, $\sigma^2_{\mathbf{x}_i}$ is its variance (equivalently
for $\mathbf{Y}$), $\sigma_{\mathbf{x}_i\mathbf{y}_i}$ is the
covariance of both sub-vols, $c_1=(k_1L) ²$, $c_2=(k_2L) 2$ are two
variables to stabilize the division with weak denominator, $L$ is the
dynamic range of the voxel values, with constants with default vales
$K_1=0.01$ , and $k_2=0.03$, and where the default size of the local
sub-vols is $7\times 7\times 7$.

\subsection{\href{https://en.wikipedia.org/wiki/Pearson_correlation_coefficient}{PPC
    (Pearson Correlation Coefficient)}}
The PPC is given by
\begin{equation}
  \text{PPC}(\mathbf{X}, \mathbf{Y}) := \frac{\sum_i(\mathbf{X}_i - \overline{\mathbf{X}})(\mathbf{Y}_i - \overline{\mathbf{Y}})}{\sqrt{\sum_i (\mathbf{X}_i - \overline{\mathbf{X}})^2 \sum_i (\mathbf{Y}_i - \overline{\mathbf{Y}})^2}}
\end{equation}

\subsection{FSC (Fourier Shell Correlation) curve}
The FSC curve is a measure of the similarity between two 3D volumes
represented in the Fourier domain \cite{verbeke2024self}. Each point
of the curve prepresents the correlation between two ``shells'' of
Fourier coefficients of both volumes. An advantage of the FSC over
other similarity metrics such as the MSE, the SSIM and the PPC is that
FSC values depend on the frequency, and this can be interesting in
some scenarios, such as microscopy, where the resolution of the
microscope if finite and known.

A FSC value of the FSC curve is determined by~\cite{verbeke2024self}
\begin{equation}
\text{FSC}(\mathbf{X}, \mathbf{Y}; r) := \frac{\sum_{i \in S_r} (\mathcal{F}(\mathbf{X)}_i \mathcal{F}(\mathbf{Y)}_i^*)}{\sqrt{\sum_{i \in S_r} |\mathcal{F}(\mathbf{X})_i|^2 \sum_{i \in S_r} |\mathcal{F}(\mathbf{Y})_i|^2}},
\end{equation}
where $i=(x, y, z)$ is a point (a coefficient in the Fourier domain)
of the surface of the sphere $S_r$ defined by $x^2+y^2+z^2=r^2$,
$\mathcal{F}(\cdot)$ is the Fourier transform of $\cdot$, and
$\cdot^*$ denotes the complex conjugate of $\cdot$.

In some fields such ashsingle particle electron cryo-microscopy
(cryo-EM), the FSC curve has become the universal resolution metric
and is used to assess the quality of a 3-D reconstruction
\cite{rosenthal2003optimal,scheres2012prevention}.

It is possible to compute the Self FSC (SFSC) by subsampling the data
\cite{koho2019fourier}, for example, splitting the volume in odd and
even slices, in each dimension (resulting a total of 6 subvolumes),
computing the FSC and averaging the correlations
\cite{verbeke2024self}. Another approach can split the data into two
random half volumes \cite{verbeke2024self}.


\bibliographystyle{plain}
\bibliography{signal_processing,microscopy,denoising}

\end{document}
